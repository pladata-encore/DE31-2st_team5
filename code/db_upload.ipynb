{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, json\n",
    "from urllib import parse\n",
    "import sqlalchemy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import preprocess\n",
    "\n",
    "\n",
    "with open('./../.API_KEY_/db_key.json', 'r') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "\n",
    "def log(msg, flag=None):\n",
    "    if flag==None:\n",
    "        flag = 0\n",
    "    head = [\"debug\", \"error\", \"status\"]\n",
    "    from time import gmtime, strftime\n",
    "    now = strftime(\"%H:%M:%S\", gmtime())\n",
    "    if not os.path.isfile(\"./debug.log\"):\n",
    "        assert subprocess.call(f\"echo \\\"[{now}][{head[flag]}] > {msg}\\\" > debug.log\", shell=True)==0, print(f\"[error] > shell command failed to execute\")\n",
    "    else: assert subprocess.call(f\"echo \\\"[{now}][{head[flag]}] > {msg}\\\" >> debug.log\", shell=True)==0, print(f\"[error] > shell command failed to execute\")\n",
    "\n",
    "\n",
    "def retrieve_df():\n",
    "    engine = establish_conn()\n",
    "    return pd.read_sql_query(\"select * from english_news_lake\", con=engine)\n",
    "\n",
    "\n",
    "def establish_conn()->sqlalchemy.Engine:\n",
    "    user = keys['user']\n",
    "    password = keys['password']\n",
    "    host = keys['ip']\n",
    "    port = keys['port']\n",
    "    database = keys['database']\n",
    "    password = parse.quote_plus(password)\n",
    "    engine = sqlalchemy.create_engine(f\"mysql://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4\")\n",
    "    return engine\n",
    "\n",
    "\n",
    "def documents_generator(df:pd.DataFrame):\n",
    "    log(f\"generating documents from dataframe...\")\n",
    "    log(f\"iteration init\")\n",
    "    for idx, row in df.iterrows():\n",
    "        log(f\"iteration index : {idx}, row : {row}\")\n",
    "        yield from row['processed_context']\n",
    "\n",
    "\n",
    "def main():\n",
    "    log(f\"retrieving dataframe from database...\")\n",
    "    df = retrieve_df()\n",
    "    # 총 문서 갯수\n",
    "    num_documents = df.shape[0]\n",
    "    # 각 문서에 대한 라벨 생성\n",
    "    document_labels = [f\"Document{x}\" for x in range(1, num_documents+1)]\n",
    "\n",
    "    # 전처리\n",
    "    # null 값 처리\n",
    "    df = preprocess.remove_null_rows(df=df).copy()\n",
    "    log(f\"df shape={df.shape}\")\n",
    "    log(f\"null values removed\")\n",
    "    # 전처리작업을 거친 df를 반환\n",
    "    log(f\"processing on rows...\")\n",
    "    df2 = preprocess.preprocess_df(df=df).copy()\n",
    "    log(f\"df2 shape={df2.shape}\")\n",
    "    log(\"processed columns assigned.\")\n",
    "    # 문서별 정제된 title + context를 기준으로 hash값 생성\n",
    "    log(f\"hash id generating...\") \n",
    "    df3 = preprocess.create_hash(df2).copy()\n",
    "    log(f\"df3 shape={df3.columns.tolist()}\")\n",
    "    log(f\"hash id assigned for each doc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\"\"\"\n",
    "변경사항\n",
    "\"\"\"\n",
    "\n",
    "log(f\"retrieving dataframe from database...\")\n",
    "df = retrieve_df()\n",
    "# 총 문서 갯수\n",
    "num_documents = df.shape[0]\n",
    "# 각 문서에 대한 라벨 생성\n",
    "document_labels = [f\"Document{x}\" for x in range(1, num_documents+1)]\n",
    "\n",
    "# 전처리\n",
    "# null 값 처리\n",
    "df = preprocess.remove_null_rows(df=df).copy()\n",
    "log(f\"df shape={df.shape}\")\n",
    "log(f\"null values removed\")\n",
    "# 전처리작업을 거친 df를 반환\n",
    "log(f\"processing on rows...\")\n",
    "df2 = preprocess.preprocess_df(df=df).copy()\n",
    "log(f\"df2 shape={df2.shape}\")\n",
    "log(\"processed columns assigned.\")\n",
    "# 문서별 정제된 title + context를 기준으로 hash값 생성\n",
    "log(f\"hash id generating...\") \n",
    "df3 = preprocess.create_hash(df2).copy()\n",
    "log(f\"df3 shape={df3.columns.tolist()}\")\n",
    "log(f\"hash id assigned for each doc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataSource',\n",
       " 'title',\n",
       " 'context',\n",
       " 'processed_title',\n",
       " 'processed_context',\n",
       " 'docKey']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['dataSource', 'title', 'context', 'docKey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[['dataSource', 'processed_title', 'processed_context', 'docKey']]\n",
    "df3.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = establish_conn()\n",
    "log(f\"complete set up engine\")\n",
    "df3 = pd.read_sql_query(\"select * from english_news_warehouse\", con=engine)\n",
    "log(f\"success download dataFrame from enligh_news_warehouse dataset\")\n",
    "tokenized_df = preprocess.create_tokens(df3).copy()\n",
    "log(f\"success tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docKey</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4178436036041573002</td>\n",
       "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>545600611569461556</td>\n",
       "      <td>dollar gains greenspan speech dollar hit highe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9421911741443619047</td>\n",
       "      <td>yukos unit buyer faces loan claim owners embat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231992186597993904</td>\n",
       "      <td>high fuel prices hit ba profits british airway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1188494042733278218</td>\n",
       "      <td>pernod takeover talk lifts domecq shares uk dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59507</th>\n",
       "      <td>9415851785762040685</td>\n",
       "      <td>kendall lays pentagon thinking future space pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59508</th>\n",
       "      <td>16147701035555579680</td>\n",
       "      <td>larger share noaa declining space budget would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59509</th>\n",
       "      <td>9119073449501516932</td>\n",
       "      <td>think tank turns attention mars two thousand s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59510</th>\n",
       "      <td>2660396055900709709</td>\n",
       "      <td>house bill leaves last three jpss satellites l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59511</th>\n",
       "      <td>3912022587547482898</td>\n",
       "      <td>championing climate change commercial weatheru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     docKey                                             tokens\n",
       "0       4178436036041573002  ad sales boost time warner profit quarterly pr...\n",
       "1        545600611569461556  dollar gains greenspan speech dollar hit highe...\n",
       "2       9421911741443619047  yukos unit buyer faces loan claim owners embat...\n",
       "3        231992186597993904  high fuel prices hit ba profits british airway...\n",
       "4       1188494042733278218  pernod takeover talk lifts domecq shares uk dr...\n",
       "...                     ...                                                ...\n",
       "59507   9415851785762040685  kendall lays pentagon thinking future space pr...\n",
       "59508  16147701035555579680  larger share noaa declining space budget would...\n",
       "59509   9119073449501516932  think tank turns attention mars two thousand s...\n",
       "59510   2660396055900709709  house bill leaves last three jpss satellites l...\n",
       "59511   3912022587547482898  championing climate change commercial weatheru...\n",
       "\n",
       "[59512 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(df3.loc[:,'title'] + df3.loc[:,'context'])\n",
    "tmp_df.columns = ['total_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float, BigInteger, DateTime, Date, Text, CHAR, String\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT\n",
    "import sqlalchemy\n",
    "from urllib import parse\n",
    "\n",
    "engine = establish_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Table('english_news_warehouse', metadata, \n",
    "      Column('dataSource', String(20)),\n",
    "      Column('title', String(255)),\n",
    "      Column('context', LONGTEXT),\n",
    "      Column('docKey', CHAR(20))\n",
    "      )\n",
    "\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Table('english_news_tokenized', metadata, \n",
    "      Column('docKey', CHAR(20)),\n",
    "      Column('tokens', LONGTEXT)\n",
    "      )\n",
    "\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.to_sql(\"english_news_warehouse\",  if_exists='append', con=engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df.to_sql(\"english_news_tokenized\",  if_exists='append', con=engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
